{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcfee360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 10:29:52.598223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import IPython.display as display\n",
    "from matplotlib import pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b2bbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root dir\n",
    "dataset_dir  = './dataset/multi-object-datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ca88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _decode at 0x7fc229cd5af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _decode at 0x7fc229cd5af0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 10:29:54.774681: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-11-23 10:29:54.779840: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-23 10:29:54.850419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:54.850987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.29GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 357.69GiB/s\n",
      "2022-11-23 10:29:54.851007: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-23 10:29:54.898731: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-23 10:29:54.898925: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-23 10:29:54.925986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-23 10:29:54.932991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-23 10:29:54.984694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-23 10:29:54.993240: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-23 10:29:55.086120: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-23 10:29:55.086488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:55.088046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:55.089354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-11-23 10:29:55.091148: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-23 10:29:55.093824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:55.095205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2070 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.29GHz coreCount: 36 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 357.69GiB/s\n",
      "2022-11-23 10:29:55.095267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-23 10:29:55.095320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-11-23 10:29:55.095361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-11-23 10:29:55.095400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-23 10:29:55.095439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-23 10:29:55.095478: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-11-23 10:29:55.095520: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-11-23 10:29:55.095560: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-23 10:29:55.095742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:55.097191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:55.098762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-11-23 10:29:55.099355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-11-23 10:29:56.501899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-23 10:29:56.501922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-11-23 10:29:56.501927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-11-23 10:29:56.502720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:56.503069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:56.503387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-23 10:29:56.503680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6476 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2070 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2022-11-23 10:29:56.505368: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "# parse clevr\n",
    "dataset = 'clevr_with_masks'\n",
    "dataset_file = 'clevr_with_masks_train.tfrecords'\n",
    "dataset_path = os.path.join(dataset_dir,dataset,dataset_file)\n",
    "IMAGE_SIZE = [240, 320]\n",
    "MAX_NUM_ENTITIES = 11\n",
    "COMPRESSION_TYPE = tf.io.TFRecordOptions.get_compression_type_string('GZIP')\n",
    "BYTE_FEATURES = ['mask', 'image', 'color', 'material', 'shape', 'size']\n",
    "features = {\n",
    "    'image': tf.io.FixedLenFeature(IMAGE_SIZE+[3], tf.string),\n",
    "    'mask': tf.io.FixedLenFeature([MAX_NUM_ENTITIES]+IMAGE_SIZE+[1], tf.string),\n",
    "    'x': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'y': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'z': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'pixel_coords': tf.io.FixedLenFeature([MAX_NUM_ENTITIES, 3], tf.float32),\n",
    "    'rotation': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'size': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.string),\n",
    "    'material': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.string),\n",
    "    'shape': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.string),\n",
    "    'color': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.string),\n",
    "    'visibility': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "}\n",
    "raw_dataset = tf.data.TFRecordDataset(dataset_path)\n",
    "\n",
    "def _decode(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the feature description dict above.\n",
    "  single_example = tf.io.parse_single_example(example_proto, features)\n",
    "  for k in BYTE_FEATURES:\n",
    "    single_example[k] = tf.squeeze(tf.io.decode_raw(single_example[k], tf.uint8),\n",
    "                                   axis=-1)\n",
    "  return single_example\n",
    "\n",
    "def dataset_loader(tfrecords_path, read_buffer_size=None, map_parallel_calls=None):\n",
    "  \"\"\"Read, decompress, and parse the TFRecords file.\n",
    "  Args:\n",
    "    tfrecords_path: str. Path to the dataset file.\n",
    "    read_buffer_size: int. Number of bytes in the read buffer. See documentation\n",
    "      for `tf.data.TFRecordDataset.__init__`.\n",
    "    map_parallel_calls: int. Number of elements decoded asynchronously in\n",
    "      parallel. See documentation for `tf.data.Dataset.map`.\n",
    "  Returns:\n",
    "    An unbatched `tf.data.TFRecordDataset`.\n",
    "  \"\"\"\n",
    "  raw_dataset = tf.data.TFRecordDataset(\n",
    "      tfrecords_path, compression_type=COMPRESSION_TYPE,\n",
    "      buffer_size=read_buffer_size)\n",
    "  return raw_dataset.map(_decode, num_parallel_calls=map_parallel_calls)\n",
    "\n",
    "parsed_image_dataset = dataset_loader(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1718d543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-23 10:30:05.810120: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-11-23 10:30:05.829393: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2199995000 Hz\n",
      "100000it [54:59, 30.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# write clevr to h5\n",
    "image_list = []\n",
    "mask_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "z_list = []\n",
    "pixel_coords_list = []\n",
    "rotation_list = []\n",
    "size_list = []\n",
    "material_list = []\n",
    "shape_list = []\n",
    "color_list = []\n",
    "visibility_list = []\n",
    "\n",
    "h5_path = dataset_path = os.path.join(dataset_dir,dataset,dataset_file.split('.')[0]+'.h5')\n",
    "with h5py.File(h5_path, 'w') as f:\n",
    "    image_group = f.create_group('image')\n",
    "    mask_group = f.create_group('mask')\n",
    "    x_group = f.create_group('x')\n",
    "    y_group = f.create_group('y')\n",
    "    z_group = f.create_group('z')\n",
    "    pixel_coords_group = f.create_group('pixel_coords')\n",
    "    rotation_group = f.create_group('rotation')\n",
    "    size_group = f.create_group('size')\n",
    "    material_group = f.create_group('material')\n",
    "    color_group = f.create_group('color')\n",
    "    visibility_group = f.create_group('visibility')\n",
    "    shape_group = f.create_group('shape')\n",
    "\n",
    "\n",
    "    for idx, image_features in tqdm(enumerate(parsed_image_dataset)):\n",
    "        key = str(idx).zfill(6)\n",
    "        image_group[key] = image_features['image'].numpy()\n",
    "        mask_group[key] = image_features['mask'].numpy()\n",
    "        x_group[key] = image_features['x'].numpy()\n",
    "        y_group[key] = image_features['y'].numpy()\n",
    "        z_group[key] = image_features['z'].numpy()\n",
    "        shape_group[key] = image_features['shape'].numpy()\n",
    "        pixel_coords_group[key] = image_features['pixel_coords'].numpy()\n",
    "        rotation_group[key] = image_features['rotation'].numpy()\n",
    "        size_group[key] = image_features['size'].numpy()\n",
    "        material_group[key] = image_features['material'].numpy()\n",
    "        color_group[key] = image_features['color'].numpy()\n",
    "        visibility_group[key] = image_features['visibility'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ebcf49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _decode at 0x7f41847795e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _decode at 0x7f41847795e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# parse multi_dsprites\n",
    "dataset = 'multi_dsprites'\n",
    "dataset_file = 'multi_dsprites_colored_on_grayscale.tfrecords'\n",
    "dataset_path = os.path.join(dataset_dir,dataset,dataset_file)\n",
    "IMAGE_SIZE = [64, 64]\n",
    "MAX_NUM_ENTITIES = {\n",
    "    'binarized': 4,\n",
    "    'colored_on_grayscale': 6,\n",
    "    'colored_on_colored': 5\n",
    "}\n",
    "COMPRESSION_TYPE = tf.io.TFRecordOptions.get_compression_type_string('GZIP')\n",
    "BYTE_FEATURES = ['mask', 'image']\n",
    "\n",
    "def feature_descriptions(max_num_entities, is_grayscale=False):\n",
    "  \"\"\"Create a dictionary describing the dataset features.\n",
    "  Args:\n",
    "    max_num_entities: int. The maximum number of foreground and background\n",
    "      entities in each image. This corresponds to the number of segmentation\n",
    "      masks and generative factors returned per scene.\n",
    "    is_grayscale: bool. Whether images are grayscale. Otherwise they're assumed\n",
    "      to be RGB.\n",
    "  Returns:\n",
    "    A dictionary which maps feature names to `tf.Example`-compatible shape and\n",
    "    data type descriptors.\n",
    "  \"\"\"\n",
    "\n",
    "  num_channels = 1 if is_grayscale else 3\n",
    "  return {\n",
    "      'image': tf.io.FixedLenFeature(IMAGE_SIZE+[num_channels], tf.string),\n",
    "      'mask': tf.io.FixedLenFeature(IMAGE_SIZE+[max_num_entities, 1], tf.string),\n",
    "      'x': tf.io.FixedLenFeature([max_num_entities], tf.float32),\n",
    "      'y': tf.io.FixedLenFeature([max_num_entities], tf.float32),\n",
    "      'shape': tf.io.FixedLenFeature([max_num_entities], tf.float32),\n",
    "      'color': tf.io.FixedLenFeature([max_num_entities, num_channels], tf.float32),\n",
    "      'visibility': tf.io.FixedLenFeature([max_num_entities], tf.float32),\n",
    "      'orientation': tf.io.FixedLenFeature([max_num_entities], tf.float32),\n",
    "      'scale': tf.io.FixedLenFeature([max_num_entities], tf.float32),\n",
    "  }\n",
    "\n",
    "def _decode(example_proto, features):\n",
    "  # Parse the input `tf.Example` proto using a feature description dictionary.\n",
    "  single_example = tf.io.parse_single_example(example_proto, features)\n",
    "  for k in BYTE_FEATURES:\n",
    "    single_example[k] = tf.squeeze(tf.io.decode_raw(single_example[k], tf.uint8),\n",
    "                                   axis=-1)\n",
    "  # To return masks in the canonical [entities, height, width, channels] format,\n",
    "  # we need to transpose the tensor axes.\n",
    "  single_example['mask'] = tf.transpose(single_example['mask'], [2, 0, 1, 3])\n",
    "  return single_example\n",
    "\n",
    "def dataset_loader(tfrecords_path, dataset_variant, read_buffer_size=None,\n",
    "            map_parallel_calls=None):\n",
    "  \"\"\"Read, decompress, and parse the TFRecords file.\n",
    "  Args:\n",
    "    tfrecords_path: str. Path to the dataset file.\n",
    "    dataset_variant: str. One of ['binarized', 'colored_on_grayscale',\n",
    "      'colored_on_colored']. This is used to identify the maximum number of\n",
    "      entities in each scene. If an incorrect identifier is passed in, the\n",
    "      TFRecords file will not be read correctly.\n",
    "    read_buffer_size: int. Number of bytes in the read buffer. See documentation\n",
    "      for `tf.data.TFRecordDataset.__init__`.\n",
    "    map_parallel_calls: int. Number of elements decoded asynchronously in\n",
    "      parallel. See documentation for `tf.data.Dataset.map`.\n",
    "  Returns:\n",
    "    An unbatched `tf.data.TFRecordDataset`.\n",
    "  \"\"\"\n",
    "  if dataset_variant not in MAX_NUM_ENTITIES:\n",
    "    raise ValueError('Invalid `dataset_variant` provided. The supported values'\n",
    "                     ' are: {}'.format(list(MAX_NUM_ENTITIES.keys())))\n",
    "  max_num_entities = MAX_NUM_ENTITIES[dataset_variant]\n",
    "  is_grayscale = dataset_variant == 'binarized'\n",
    "  raw_dataset = tf.data.TFRecordDataset(\n",
    "      tfrecords_path, compression_type=COMPRESSION_TYPE,\n",
    "      buffer_size=read_buffer_size)\n",
    "  features = feature_descriptions(max_num_entities, is_grayscale)\n",
    "  partial_decode_fn = functools.partial(_decode, features=features)\n",
    "  return raw_dataset.map(partial_decode_fn,\n",
    "                         num_parallel_calls=map_parallel_calls)\n",
    "\n",
    "\n",
    "\n",
    "parsed_image_dataset = dataset_loader(dataset_path,'colored_on_grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bffda53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80001it [03:51, 345.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# write multi_dsprites to h5\n",
    "image_list = []\n",
    "mask_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "shape_list = []\n",
    "color_list = []\n",
    "visibility_list = []\n",
    "orientation_list = []\n",
    "scale_list = []\n",
    "\n",
    "h5_path = dataset_path = os.path.join(dataset_dir,dataset,dataset_file.split('.')[0]+'.h5')\n",
    "with h5py.File(h5_path, 'w') as f:\n",
    "    image_group = f.create_group('image')\n",
    "    mask_group = f.create_group('mask')\n",
    "    x_group = f.create_group('x')\n",
    "    y_group = f.create_group('y')\n",
    "    shape_group = f.create_group('shape')\n",
    "    color_group = f.create_group('color')\n",
    "    visibility_group = f.create_group('visibility')\n",
    "    orientation_group = f.create_group('orientation')\n",
    "    scale_group = f.create_group('scale')\n",
    "\n",
    "    for idx, image_features in tqdm(enumerate(parsed_image_dataset)):\n",
    "        if idx >80000:\n",
    "            break\n",
    "        key = str(idx).zfill(6)\n",
    "        image_group[key] = image_features['image'].numpy()\n",
    "        mask_group[key] = image_features['mask'].numpy()\n",
    "        x_group[key] = image_features['x'].numpy()\n",
    "        y_group[key] = image_features['y'].numpy()\n",
    "        shape_group[key] = image_features['shape'].numpy()\n",
    "        color_group[key] = image_features['color'].numpy()\n",
    "        visibility_group[key] = image_features['visibility'].numpy()\n",
    "        orientation_group[key] = image_features['orientation'].numpy()\n",
    "        scale_group[key] = image_features['scale'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41ac32dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _decode at 0x7f41f006dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function _decode at 0x7f41f006dee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# parse tetrominoes\n",
    "dataset = 'tetrominoes'\n",
    "dataset_file = 'tetrominoes_train.tfrecords'\n",
    "dataset_path = os.path.join(dataset_dir,dataset,dataset_file)\n",
    "COMPRESSION_TYPE = tf.io.TFRecordOptions.get_compression_type_string('GZIP')\n",
    "IMAGE_SIZE = [35, 35]\n",
    "# The maximum number of foreground and background entities in the provided\n",
    "# dataset. This corresponds to the number of segmentation masks returned per\n",
    "# scene.\n",
    "MAX_NUM_ENTITIES = 4\n",
    "BYTE_FEATURES = ['mask', 'image']\n",
    "\n",
    "# Create a dictionary mapping feature names to `tf.Example`-compatible\n",
    "# shape and data type descriptors.\n",
    "features = {\n",
    "    'image': tf.io.FixedLenFeature(IMAGE_SIZE+[3], tf.string),\n",
    "    'mask': tf.io.FixedLenFeature([MAX_NUM_ENTITIES]+IMAGE_SIZE+[1], tf.string),\n",
    "    'x': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'y': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'shape': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "    'color': tf.io.FixedLenFeature([MAX_NUM_ENTITIES, 3], tf.float32),\n",
    "    'visibility': tf.io.FixedLenFeature([MAX_NUM_ENTITIES], tf.float32),\n",
    "}\n",
    "\n",
    "def _decode(example_proto):\n",
    "  # Parse the input `tf.Example` proto using the feature description dict above.\n",
    "  single_example = tf.io.parse_single_example(example_proto, features)\n",
    "  for k in BYTE_FEATURES:\n",
    "    single_example[k] = tf.squeeze(tf.io.decode_raw(single_example[k], tf.uint8),\n",
    "                                   axis=-1)\n",
    "  return single_example\n",
    "\n",
    "\n",
    "def dataset_loader(tfrecords_path, read_buffer_size=None, map_parallel_calls=None):\n",
    "  \"\"\"Read, decompress, and parse the TFRecords file.\n",
    "  Args:\n",
    "    tfrecords_path: str. Path to the dataset file.\n",
    "    read_buffer_size: int. Number of bytes in the read buffer. See documentation\n",
    "      for `tf.data.TFRecordDataset.__init__`.\n",
    "    map_parallel_calls: int. Number of elements decoded asynchronously in\n",
    "      parallel. See documentation for `tf.data.Dataset.map`.\n",
    "  Returns:\n",
    "    An unbatched `tf.data.TFRecordDataset`.\n",
    "  \"\"\"\n",
    "  raw_dataset = tf.data.TFRecordDataset(\n",
    "      tfrecords_path, compression_type=COMPRESSION_TYPE,\n",
    "      buffer_size=read_buffer_size)\n",
    "  return raw_dataset.map(_decode, num_parallel_calls=map_parallel_calls)\n",
    "\n",
    "parsed_image_dataset = dataset_loader(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fff5cc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80001it [02:30, 530.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# write tetrominoes to h5\n",
    "image_list = []\n",
    "mask_list = []\n",
    "x_list = []\n",
    "y_list = []\n",
    "shape_list = []\n",
    "color_list = []\n",
    "visibility_list = []\n",
    "\n",
    "\n",
    "h5_path = dataset_path = os.path.join(dataset_dir,dataset,dataset_file.split('.')[0]+'.h5')\n",
    "with h5py.File(h5_path, 'w') as f:\n",
    "    image_group = f.create_group('image')\n",
    "    mask_group = f.create_group('mask')\n",
    "    x_group = f.create_group('x')\n",
    "    y_group = f.create_group('y')\n",
    "    shape_group = f.create_group('shape')\n",
    "    color_group = f.create_group('color')\n",
    "    visibility_group = f.create_group('visibility')\n",
    "\n",
    "\n",
    "    for idx, image_features in tqdm(enumerate(parsed_image_dataset)):\n",
    "        if idx >80000:\n",
    "            break\n",
    "        key = str(idx).zfill(6)\n",
    "        image_group[key] = image_features['image'].numpy()\n",
    "        mask_group[key] = image_features['mask'].numpy()\n",
    "        x_group[key] = image_features['x'].numpy()\n",
    "        y_group[key] = image_features['y'].numpy()\n",
    "        shape_group[key] = image_features['shape'].numpy()\n",
    "        color_group[key] = image_features['color'].numpy()\n",
    "        visibility_group[key] = image_features['visibility'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a4940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
